{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-antimonious/GRU-Emotion-Classification/blob/main/RUSU_Projekt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GJozY_LRt0w"
      },
      "source": [
        "# Treniranje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkDlTNq4fRXE"
      },
      "outputs": [],
      "source": [
        "##### IMPORTI, KONSTANTE, UTILITIES\n",
        "\n",
        "%%capture\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import shutil\n",
        "import math\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import librosa\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "import seaborn as sns\n",
        "\n",
        "##### PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "\n",
        "DRIVE =                 '/content/drive/MyDrive/LSSED/'   # KONSTANTA ZA DRIVE FOLDER GDJE JE POHRANJENO SVE ZA PROJEKT\n",
        "WAV2VEC2_LAST =         '/content/Wav2Vec2_LAST/'         # KONSTANTA ZA FOLDER GDJE SU RASPAKIRANI WA2VEC2 FEATURES ZADNJEG SLOJA\n",
        "WAV2VEC2_LAST_TRAIN =   WAV2VEC2_LAST + 'TRAIN/'            # ZA TRENIRANJE\n",
        "WAV2VEC2_LAST_EVAL =    WAV2VEC2_LAST + 'EVAL/'             # ZA EVALUACIJU\n",
        "WAV2VEC2_SAMPLE =       '/content/content/SAMPLE/'        # KONSTANTA ZA FOLDER GDJE SU RASPAKIRANI UZORCI WAV2VEC2 FEATURES-A\n",
        "TRAIN_METADATA =        DRIVE + 'train_metadata.csv'      # KONSTANTA ZA TRAIN DATA FILE\n",
        "EVAL_METADATA =         DRIVE + 'eval_metadata.csv'       # KONSTANTA ZA EVALUATION DATA FILE\n",
        "SAMPLE_METADATA =       DRIVE + 'sample_metadata.csv'     # KONSTANTA ZA SAMPLE DATA FILE\n",
        "WAV2VEC2_LAST_PACKED =  DRIVE + 'Wav2Vec2_LAST.tar.gz'    # KONSTANTA ZA ZAPAKIRANE WAV2VEC2 FEATURES ZADNJEG SLOJA\n",
        "WAV2VEC2_SAM_PACKED =   DRIVE + 'Wav2Vec2_SAMPLE.tar.gz'  # KONSTANTA ZA ZAPAKIRANE UZORKE WAV2VEC2 FEATURES-A\n",
        "WAV2VEC2_NAME =         'facebook/wav2vec2-large-xlsr-53' # KONSTANTA ZA IME WAV2VEC2 PRE-TRAINED MODELA\n",
        "SAMPLING_RATE =         48000                             # KONSTANTA ZA SAMPLING RATE SNIMAKA\n",
        "BATCH_SIZE =            128                               # KONSTANTA ZA VELIČINU BATCHA\n",
        "DEVICE =                torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "WAV2VEC2_LAST_TUPLE =   (WAV2VEC2_LAST, WAV2VEC2_LAST_TRAIN, WAV2VEC2_LAST_EVAL, WAV2VEC2_LAST_PACKED, TRAIN_METADATA, EVAL_METADATA)\n",
        "WAV2VEC2_SAM_TUPLE =    (WAV2VEC2_SAMPLE, WAV2VEC2_SAMPLE, WAV2VEC2_SAMPLE, WAV2VEC2_SAM_PACKED, SAMPLE_METADATA, SAMPLE_METADATA)\n",
        "\n",
        "#torch.backends.cudnn.deterministic = True\n",
        "#torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_Xx4gNVFB9T"
      },
      "outputs": [],
      "source": [
        "##### BRISANJE GOOGLE COLAB DEFAULT DATOTEKA\n",
        "\n",
        "try:\n",
        "  del_files = os.listdir('/content/sample_data/')\n",
        "  for string in del_files:\n",
        "    if os.path.exists(\"/content/sample_data/\"+string):\n",
        "      os.remove(\"/content/sample_data/\"+string)\n",
        "    else:\n",
        "      print(\"Failed to remove \"+string)\n",
        "\n",
        "  if os.path.exists(\"/content/sample_data/\"):\n",
        "    os.rmdir(\"/content/sample_data/\")\n",
        "  else:\n",
        "    print(\"Failed to remove sample_data directory\")\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR_vSrVIas_H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQW1m3JxGkKR"
      },
      "outputs": [],
      "source": [
        "##### ODABIR DATASET-A\n",
        "\n",
        "print('---------- PLEASE CHOOSE A DATASET TO BE USED IN THIS SESSION ----------')\n",
        "print('Type \"LAST\" for LSSED dataset of features extracted from the final layer')\n",
        "print('Type \"TEST\" for 1.2k sample LSSED dataset - ONLY FOR TESTING PURPOSES')\n",
        "print('---------- PLEASE CHOOSE A DATASET TO BE USED IN THIS SESSION ----------')\n",
        "\n",
        "while True:\n",
        "  choice = input('Type in your choice: ')\n",
        "  choice = choice.upper()\n",
        "\n",
        "  if choice == 'LAST':\n",
        "    WAV2VEC2_TUPLE = WAV2VEC2_LAST_TUPLE\n",
        "    break\n",
        "  elif choice == 'TEST':\n",
        "    WAV2VEC2_TUPLE = WAV2VEC2_SAM_TUPLE\n",
        "    break\n",
        "  else:\n",
        "    print(choice, 'is not a correct input.')\n",
        "\n",
        "print('You chose:', choice)\n",
        "print(WAV2VEC2_TUPLE)\n",
        "\n",
        "WAV2VEC2, WAV2VEC2_TRAIN, WAV2VEC2_EVAL, WAV2VEC2_PACKED, TRAIN_METADATA_PATH, EVAL_METADATA_PATH = WAV2VEC2_TUPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy5_lWQNdQC-"
      },
      "outputs": [],
      "source": [
        "##### RASPAKIRAVANJE DATASET-A ZA KORIŠTENJE U COLAB-U\n",
        "\n",
        "file = tarfile.open(WAV2VEC2_PACKED, 'r:gz')\n",
        "file.extractall('/content/')\n",
        "file.close()\n",
        "\n",
        "del file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e97kyXVMhLp-"
      },
      "outputs": [],
      "source": [
        "##### INICIJALIZACIJA DATAFRAME-OVA KOJI POVEZUJU VVID-OVE FILE-OVA I EMOCIJE (LABELE)\n",
        "\n",
        "train_metadata =  pd.read_csv(TRAIN_METADATA_PATH).drop(\"Unnamed: 0\", axis = 1).reset_index().drop(\"index\", axis = 1)\n",
        "eval_metadata =   pd.read_csv(EVAL_METADATA_PATH).drop(\"Unnamed: 0\", axis = 1).reset_index().drop(\"index\", axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH7CK_VC8EjY"
      },
      "outputs": [],
      "source": [
        "##### DEFINIRANJE KATEGORIČKIH VRIJEDNOSTI\n",
        "\n",
        "train_metadata['Age'] =     train_metadata['Age'].astype('category')\n",
        "train_metadata['Gender'] =  train_metadata['Gender'].astype('category')\n",
        "train_metadata['Emotion'] = train_metadata['Emotion'].astype('category')\n",
        "\n",
        "eval_metadata['Age'] =      eval_metadata['Age'].astype('category')\n",
        "eval_metadata['Gender'] =   eval_metadata['Gender'].astype('category')\n",
        "eval_metadata['Emotion'] =  eval_metadata['Emotion'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRnWaNRR7qge"
      },
      "outputs": [],
      "source": [
        "##### KODIRANJE KATEGORIČKIH VRIJEDNOSTI\n",
        "\n",
        "encoded_train =  pd.get_dummies(train_metadata['Emotion'])\n",
        "encoded_eval =   pd.get_dummies(eval_metadata['Emotion'])\n",
        "\n",
        "train_metadata = pd.concat([train_metadata, encoded_train], axis = 1)\n",
        "eval_metadata = pd.concat([eval_metadata, encoded_eval], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on1c6aSjF4a_"
      },
      "outputs": [],
      "source": [
        "##### KONVERTIRANJE IZ MAT FORMATA U ARRAY\n",
        "\n",
        "def mat_to_array(mat_format):\n",
        "\n",
        "  feature_array = [[element for element in upperElement] for upperElement in mat_format['w2v2']]\n",
        "  return feature_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1rMOEkbFbX6"
      },
      "outputs": [],
      "source": [
        "##### UČITAVANJE FEATURE ARRAYA\n",
        "\n",
        "def load_feature_array(mat_file):\n",
        "\n",
        "  mat_format = loadmat(mat_file)\n",
        "  feature_array = mat_to_array(mat_format)\n",
        "  feature_array = torch.FloatTensor(feature_array)\n",
        "  return feature_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "oPvJc_Cphuvh"
      },
      "outputs": [],
      "source": [
        "##### DEFINIRANJE CUSTOM DATASETA\n",
        "\n",
        "class LSSED_Dataset(Dataset):\n",
        "  \"\"\"LSSED dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, metadata, directory, transform = None):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        metadata (DataFrame):             Pandas DataFrame containing dataset information.\n",
        "        directory (string):               Path to the directory with the feature array files.\n",
        "        transform (class | list | None):  Data transformation options.\n",
        "    \"\"\"\n",
        "    self.metadata = metadata\n",
        "    self.directory = directory\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.metadata)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    path = self.directory + self.metadata['Vvid'][idx]\n",
        "    emotion = [self.metadata['Angry'][idx], self.metadata['Happy'][idx], self.metadata['Neutral'][idx], self.metadata['Sad'][idx], self.metadata['Disgusted'][idx]]\n",
        "\n",
        "    sample = {'path': path, 'emotion': emotion}\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxROpy0yrUlw"
      },
      "outputs": [],
      "source": [
        "##### INICIJALIZACIJA DATASETOVA\n",
        "\n",
        "train_dataset = LSSED_Dataset(train_metadata, WAV2VEC2_TRAIN)\n",
        "eval_dataset =  LSSED_Dataset(eval_metadata, WAV2VEC2_EVAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fA2BwB7Kg59"
      },
      "outputs": [],
      "source": [
        "##### NOVA METODA ZA PADDING\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "  data = [load_feature_array(batch[i]['path']) for i in range(batch.__len__())]\n",
        "  target = [batch[i]['emotion'] for i in range(batch.__len__())]\n",
        "  length = [data[i].shape[0] for i in range(batch.__len__())]\n",
        "\n",
        "  data = pad_sequence(data, True, 0)\n",
        "  target = torch.as_tensor(target)\n",
        "\n",
        "  return (data, target, length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpkziW1dsvZL"
      },
      "outputs": [],
      "source": [
        "##### INICIJALIZACIJA DATALOADERA\n",
        "\n",
        "train_dataloader =  DataLoader(train_dataset, BATCH_SIZE, True, num_workers = 12, collate_fn = collate_fn)\n",
        "eval_dataloader =   DataLoader(eval_dataset, BATCH_SIZE, False, num_workers = 12, collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgjPRH08Qqn7"
      },
      "outputs": [],
      "source": [
        "##### DEFINIRANJE KLASIFIKATORA\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm = nn.LayerNorm(normalized_shape = 1024)\n",
        "    self.rnn1 = nn.LSTM(input_size = 1024, hidden_size = 1024,\n",
        "                        num_layers = 3, batch_first = True,\n",
        "                        bidirectional = False)\n",
        "    self.linear1 = nn.Linear(1024, 5)\n",
        "\n",
        "  def forward(self, x, length):\n",
        "    out = self.norm(x)\n",
        "    out, _ = self.rnn1(out)\n",
        "\n",
        "    # Many-to-one RNN mod\n",
        "    try:\n",
        "      _ = out.shape[2]\n",
        "      indices = [i for i in range(out.shape[0])]\n",
        "      out = out[indices, np.subtract(length, 1), :]\n",
        "    except:\n",
        "      out = out[np.subtract(length, 1), :]\n",
        "\n",
        "    out = self.linear1(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3OK6tzRhHRC"
      },
      "outputs": [],
      "source": [
        "##### INICIJALIZACIJA KLASIFIKATORA\n",
        "\n",
        "model = EmotionClassifier().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct56RTEeVbyd"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, loss_module, val_data_loader):\n",
        "  ##### Set model to eval mode\n",
        "  model.eval()\n",
        "\n",
        "  ##### Initializing necessary variables\n",
        "  true_preds, num_preds, eval_loss = 0.0, 0.0, 0.0\n",
        "  true_zeros, true_ones, true_twos, true_threes, true_fours = 0.0, 0.0, 0.0, 0.0, 0.0 # TP\n",
        "  false_zeros, false_ones, false_twos, false_threes, false_fours = 0.0, 0.0, 0.0, 0.0, 0.0 # FP\n",
        "  missed_zeros, missed_ones, missed_twos, missed_threes, missed_fours = 0.0, 0.0, 0.0, 0.0, 0.0 # FN\n",
        "\n",
        "  ##### Deactivate gradients\n",
        "  with torch.no_grad():\n",
        "    for data_inputs, labels, lengths in val_data_loader:\n",
        "\n",
        "      ##### Moving data to device\n",
        "      data_inputs = data_inputs.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      _, labels = torch.max(labels.data, dim = 1)\n",
        "\n",
        "      ##### Running the model on the input data\n",
        "      output = model(data_inputs, lengths)\n",
        "\n",
        "      ##### Calculating loss\n",
        "      loss = loss_module(output, labels)\n",
        "\n",
        "      ##### Calculating precision\n",
        "      _, pred_labels = torch.max(output.data, dim = 1)\n",
        "\n",
        "      true_preds += (pred_labels == labels).sum().item()\n",
        "\n",
        "      true_zeros +=   torch.sum((pred_labels == 0) & (labels == 0)).item()\n",
        "      true_ones +=    torch.sum((pred_labels == 1) & (labels == 1)).item()\n",
        "      true_twos +=    torch.sum((pred_labels == 2) & (labels == 2)).item()\n",
        "      true_threes +=  torch.sum((pred_labels == 3) & (labels == 3)).item()\n",
        "      true_fours +=   torch.sum((pred_labels == 4) & (labels == 4)).item()\n",
        "\n",
        "      false_zeros +=  torch.sum((pred_labels == 0) & (labels != 0)).item()\n",
        "      false_ones +=   torch.sum((pred_labels == 1) & (labels != 1)).item()\n",
        "      false_twos +=   torch.sum((pred_labels == 2) & (labels != 2)).item()\n",
        "      false_threes += torch.sum((pred_labels == 3) & (labels != 3)).item()\n",
        "      false_fours +=  torch.sum((pred_labels == 4) & (labels != 4)).item()\n",
        "\n",
        "      missed_zeros +=   torch.sum((pred_labels != 0) & (labels == 0)).item()\n",
        "      missed_ones +=    torch.sum((pred_labels != 1) & (labels == 1)).item()\n",
        "      missed_twos +=    torch.sum((pred_labels != 2) & (labels == 2)).item()\n",
        "      missed_threes +=  torch.sum((pred_labels != 3) & (labels == 3)).item()\n",
        "      missed_fours +=   torch.sum((pred_labels != 4) & (labels == 4)).item()\n",
        "\n",
        "      num_preds += labels.shape[0]\n",
        "\n",
        "      eval_loss += loss.item() * len(data_inputs)\n",
        "\n",
        "      ##### Cleaning up\n",
        "      del data_inputs, labels, output, loss, pred_labels\n",
        "\n",
        "  ##### Various processing\n",
        "  eval_loss /= len(val_data_loader.dataset)\n",
        "  eval_acc = true_preds / num_preds\n",
        "\n",
        "  zeros_weight =  1164. / len(val_data_loader.dataset)\n",
        "  ones_weight =   1181. / len(val_data_loader.dataset)\n",
        "  twos_weight =   1200. / len(val_data_loader.dataset)\n",
        "  threes_weight = 1205. / len(val_data_loader.dataset)\n",
        "  fours_weight =  632. / len(val_data_loader.dataset)\n",
        "\n",
        "  prec_zeros =  true_zeros / (true_zeros + false_zeros + 1e-10)\n",
        "  prec_ones =   true_ones / (true_ones + false_ones + 1e-10)\n",
        "  prec_twos =   true_twos / (true_twos + false_twos + 1e-10)\n",
        "  prec_threes = true_threes / (true_threes + false_threes + 1e-10)\n",
        "  prec_fours =  true_fours / (true_fours + false_fours + 1e-10)\n",
        "\n",
        "  rec_zeros =   true_zeros / (true_zeros + missed_zeros + 1e-10)\n",
        "  rec_ones =    true_ones / (true_ones + missed_ones + 1e-10)\n",
        "  rec_twos =    true_twos / (true_twos + missed_twos + 1e-10)\n",
        "  rec_threes =  true_threes / (true_threes + missed_threes + 1e-10)\n",
        "  rec_fours =   true_fours / (true_fours + missed_fours + 1e-10)\n",
        "\n",
        "  f1_zeros =  (2 * prec_zeros * rec_zeros) / (prec_zeros + rec_zeros + 1e-10)\n",
        "  f1_ones =   (2 * prec_ones * rec_ones) / (prec_ones + rec_ones + 1e-10)\n",
        "  f1_twos =   (2 * prec_twos * rec_twos) / (prec_twos + rec_twos + 1e-10)\n",
        "  f1_threes = (2 * prec_threes * rec_threes) / (prec_threes + rec_threes + 1e-10)\n",
        "  f1_fours =  (2 * prec_fours * rec_fours) / (prec_fours + rec_fours + 1e-10)\n",
        "\n",
        "  eval_prec =  zeros_weight * prec_zeros + ones_weight * prec_ones + twos_weight * prec_twos + threes_weight * prec_threes + fours_weight * prec_fours\n",
        "  eval_rec =   zeros_weight * rec_zeros + ones_weight * rec_ones + twos_weight * rec_twos + threes_weight * rec_threes + fours_weight * rec_fours\n",
        "  eval_f1 =    zeros_weight * f1_zeros + ones_weight * f1_ones + twos_weight * f1_twos + threes_weight * f1_threes + fours_weight * f1_fours\n",
        "\n",
        "  print(\"Eval zeros:\", str(true_zeros + false_zeros))\n",
        "  print(\"Eval ones:\", str(true_ones + false_ones))\n",
        "  print(\"Eval twos:\", str(true_twos + false_twos))\n",
        "  print(\"Eval threes:\", str(true_threes + false_threes))\n",
        "  print(\"Eval fours:\", str(true_fours + false_fours))\n",
        "\n",
        "  return eval_loss, eval_acc, eval_prec, eval_rec, eval_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN0GhMoCja4z"
      },
      "outputs": [],
      "source": [
        "def train_model_with_logger(model, loss_module, optimizer, scheduler, train_data_loader, val_data_loader, num_epochs=100, start_epoch=0, logging_dir='runs/our_experiment', name='task_1'):\n",
        "\n",
        "  ##### Set model to train mode\n",
        "  writer = SummaryWriter(logging_dir)\n",
        "  %tensorboard --logdir runs/our_experiment\n",
        "  model.train()\n",
        "\n",
        "  ##### Saving each epoch as checkpoint\n",
        "  if os.path.exists('/content/' + name + '/') == False:\n",
        "    os.mkdir('/content/' + name + '/')\n",
        "\n",
        "  ##### Training loop\n",
        "  for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "    ##### Initializing necessary variables\n",
        "    true_preds, num_preds, epoch_loss = 0.0, 0.0, 0.0\n",
        "    true_zeros, true_ones, true_twos, true_threes, true_fours = 0.0, 0.0, 0.0, 0.0, 0.0 # TP\n",
        "    false_zeros, false_ones, false_twos, false_threes, false_fours = 0.0, 0.0, 0.0, 0.0, 0.0 # FP\n",
        "    missed_zeros, missed_ones, missed_twos, missed_threes, missed_fours = 0.0, 0.0, 0.0, 0.0, 0.0 # FN\n",
        "\n",
        "    for data_inputs, labels, lengths in tqdm(train_data_loader, 'Epoch %d'%(epoch + 1)):\n",
        "\n",
        "      ##### Moving data to device\n",
        "      data_inputs = data_inputs.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      _, labels = torch.max(labels.data, dim = 1)\n",
        "\n",
        "      ##### Running the model on the input data\n",
        "      output = model(data_inputs, lengths)\n",
        "\n",
        "      ##### Calculating loss\n",
        "      loss = loss_module(output, labels)\n",
        "\n",
        "      ##### Calculating precision\n",
        "      _, pred_labels = torch.max(output.data, dim = 1)\n",
        "\n",
        "      true_preds += (pred_labels == labels).sum().item()\n",
        "\n",
        "      true_zeros +=   torch.sum((pred_labels == 0) & (labels == 0)).item()\n",
        "      true_ones +=    torch.sum((pred_labels == 1) & (labels == 1)).item()\n",
        "      true_twos +=    torch.sum((pred_labels == 2) & (labels == 2)).item()\n",
        "      true_threes +=  torch.sum((pred_labels == 3) & (labels == 3)).item()\n",
        "      true_fours +=   torch.sum((pred_labels == 4) & (labels == 4)).item()\n",
        "\n",
        "      false_zeros +=  torch.sum((pred_labels == 0) & (labels != 0)).item()\n",
        "      false_ones +=   torch.sum((pred_labels == 1) & (labels != 1)).item()\n",
        "      false_twos +=   torch.sum((pred_labels == 2) & (labels != 2)).item()\n",
        "      false_threes += torch.sum((pred_labels == 3) & (labels != 3)).item()\n",
        "      false_fours +=  torch.sum((pred_labels == 4) & (labels != 4)).item()\n",
        "\n",
        "      missed_zeros +=   torch.sum((pred_labels != 0) & (labels == 0)).item()\n",
        "      missed_ones +=    torch.sum((pred_labels != 1) & (labels == 1)).item()\n",
        "      missed_twos +=    torch.sum((pred_labels != 2) & (labels == 2)).item()\n",
        "      missed_threes +=  torch.sum((pred_labels != 3) & (labels == 3)).item()\n",
        "      missed_fours +=   torch.sum((pred_labels != 4) & (labels == 4)).item()\n",
        "\n",
        "      num_preds += labels.shape[0]\n",
        "\n",
        "      ##### Propagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item() * len(data_inputs)\n",
        "\n",
        "      ##### Cleaning up\n",
        "      del data_inputs, labels, output, loss, pred_labels\n",
        "\n",
        "    ##### Metrics\n",
        "    epoch_loss /= len(train_data_loader.dataset)\n",
        "    epoch_acc =   true_preds / num_preds\n",
        "\n",
        "    zeros_weight =  4663. / len(train_data_loader.dataset)\n",
        "    ones_weight =   4724. / len(train_data_loader.dataset)\n",
        "    twos_weight =   4800. / len(train_data_loader.dataset)\n",
        "    threes_weight = 4822. / len(train_data_loader.dataset)\n",
        "    fours_weight =  2536. / len(train_data_loader.dataset)\n",
        "\n",
        "    prec_zeros =  true_zeros / (true_zeros + false_zeros + 1e-10)\n",
        "    prec_ones =   true_ones / (true_ones + false_ones + 1e-10)\n",
        "    prec_twos =   true_twos / (true_twos + false_twos + 1e-10)\n",
        "    prec_threes = true_threes / (true_threes + false_threes + 1e-10)\n",
        "    prec_fours =  true_fours / (true_fours + false_fours + 1e-10)\n",
        "\n",
        "    rec_zeros =   true_zeros / (true_zeros + missed_zeros + 1e-10)\n",
        "    rec_ones =    true_ones / (true_ones + missed_ones + 1e-10)\n",
        "    rec_twos =    true_twos / (true_twos + missed_twos + 1e-10)\n",
        "    rec_threes =  true_threes / (true_threes + missed_threes + 1e-10)\n",
        "    rec_fours =   true_fours / (true_fours + missed_fours + 1e-10)\n",
        "\n",
        "    f1_zeros =  (2 * prec_zeros * rec_zeros) / (prec_zeros + rec_zeros + 1e-10)\n",
        "    f1_ones =   (2 * prec_ones * rec_ones) / (prec_ones + rec_ones + 1e-10)\n",
        "    f1_twos =   (2 * prec_twos * rec_twos) / (prec_twos + rec_twos + 1e-10)\n",
        "    f1_threes = (2 * prec_threes * rec_threes) / (prec_threes + rec_threes + 1e-10)\n",
        "    f1_fours =  (2 * prec_fours * rec_fours) / (prec_fours + rec_fours + 1e-10)\n",
        "\n",
        "    epoch_prec =  zeros_weight * prec_zeros + ones_weight * prec_ones + twos_weight * prec_twos + threes_weight * prec_threes + fours_weight * prec_fours\n",
        "    epoch_rec =   zeros_weight * rec_zeros + ones_weight * rec_ones + twos_weight * rec_twos + threes_weight * rec_threes + fours_weight * rec_fours\n",
        "    epoch_f1 =    zeros_weight * f1_zeros + ones_weight * f1_ones + twos_weight * f1_twos + threes_weight * f1_threes + fours_weight * f1_fours\n",
        "\n",
        "    eval_loss, eval_acc, eval_prec, eval_rec, eval_f1 = eval_model(model, loss_module, val_data_loader)\n",
        "\n",
        "    print(f\"Training loss: {epoch_loss:4.5f}\")\n",
        "    print(f\"Training accuracy: {100.0*epoch_acc:4.5f}%\")\n",
        "    print(f\"Training precision: {100.0*epoch_prec:4.5f}%\")\n",
        "    print(f\"Training recall: {100.0*epoch_rec:4.5f}%\")\n",
        "    print(f\"Training F1-score: {100.0*epoch_f1:4.5f}%\")\n",
        "    print(f\"Eval loss: {eval_loss:4.5f}\")\n",
        "    print(f\"Eval accuracy: {100.0*eval_acc:4.5f}%\")\n",
        "    print(f\"Eval precision: {100.0*eval_prec:4.5f}%\")\n",
        "    print(f\"Eval recall: {100.0*eval_rec:4.5f}%\")\n",
        "    print(f\"Eval F1-score: {100.0*eval_f1:4.5f}%\")\n",
        "\n",
        "    print(\"Train zeros:\", str(true_zeros + false_zeros))\n",
        "    print(\"Train ones:\", str(true_ones + false_ones))\n",
        "    print(\"Train twos:\", str(true_twos + false_twos))\n",
        "    print(\"Train threes:\", str(true_threes + false_threes))\n",
        "    print(\"Train fours:\", str(true_fours + false_fours))\n",
        "\n",
        "    writer.add_scalar('training_loss', epoch_loss, global_step = epoch + 1)\n",
        "    writer.add_scalar('training_acc', epoch_acc, global_step = epoch + 1)\n",
        "    writer.add_scalar('training_prec', epoch_prec, global_step = epoch + 1)\n",
        "    writer.add_scalar('training_rec', epoch_rec, global_step = epoch + 1)\n",
        "    writer.add_scalar('training_f1_score', epoch_f1, global_step = epoch + 1)\n",
        "    writer.add_scalar('eval_loss', eval_loss, global_step = epoch + 1)\n",
        "    writer.add_scalar('eval_acc', eval_acc, global_step = epoch + 1)\n",
        "    writer.add_scalar('eval_prec', eval_prec, global_step = epoch + 1)\n",
        "    writer.add_scalar('eval_rec', eval_rec, global_step = epoch + 1)\n",
        "    writer.add_scalar('eval_f1_score', eval_f1, global_step = epoch + 1)\n",
        "\n",
        "    state_dict = model.state_dict()\n",
        "    torch.save(state_dict, \"/content/\" + name + \"/model_\" + name + '_' + str(epoch+1) + \".pt\")\n",
        "    model.train()\n",
        "    scheduler.step()\n",
        "\n",
        "  writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkanNRPbpdAO"
      },
      "outputs": [],
      "source": [
        "loss_module = nn.CrossEntropyLoss(weight = torch.FloatTensor(\n",
        "    [21545./4663., 21545./4724., 21545./4800.,\n",
        "     21545./4822., 21545./2536.]).to(DEVICE),\n",
        "                                  label_smoothing = 0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00005,\n",
        "                             amsgrad = True, fused = True)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                            step_size = 25, gamma = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KAHpGz07iVy"
      },
      "outputs": [],
      "source": [
        "train_model_with_logger(model, loss_module, optimizer, scheduler, train_dataloader, eval_dataloader, num_epochs = 150, name = 'LSSED_RNN_experiment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcKFrII_Y3Of"
      },
      "source": [
        "# Svaštara"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnS17yeP179f",
        "outputId": "a1acb8b0-dd71-419e-aebf-c62e7b0b0781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "501"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EWGtxcIRd9N"
      },
      "source": [
        "# Za aplikaciju"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-DNy9AU97ha"
      },
      "outputs": [],
      "source": [
        "##### PODEŠAVANJE WAV2VEC2\n",
        "\n",
        "FEATURE_EXTRACTOR = transformers.Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_NAME)\n",
        "WAV2VEC2_MODEL =    transformers.Wav2Vec2Model.from_pretrained(WAV2VEC2_NAME).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1-TZ3snMErG"
      },
      "outputs": [],
      "source": [
        "##### DEFINIRANJE KLASIFIKATORA\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.rnn1 = nn.RNN(input_size = 1024, hidden_size = 1024, num_layers = 8, batch_first = True, bidirectional = False)\n",
        "    self.linear1 = nn.Linear(1024, 4)\n",
        "\n",
        "  def forward(self, x, length):\n",
        "    out, _ = self.rnn1(x)\n",
        "\n",
        "    # Many-to-one RNN mod\n",
        "    try:\n",
        "      _ = out.shape[2]\n",
        "      indices = [i for i in range(out.shape[0])]\n",
        "      out = out[indices, np.subtract(length, 1), :]\n",
        "    except:\n",
        "      out = out[np.subtract(length, 1), :]\n",
        "\n",
        "    out = self.linear1(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdRAQqJATVsI"
      },
      "outputs": [],
      "source": [
        "##### DEFINIRANJE CIJELOG MODELA\n",
        "\n",
        "class Emotioner(nn.Module):\n",
        "  def __init__(self, feature_extractor, wav2vec2_model, emotion_classifier, sampling_rate = 16000):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.wav2vec2_model = wav2vec2_model\n",
        "    self.emotion_classifier = emotion_classifier\n",
        "    self.sampling_rate = sampling_rate\n",
        "    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "  def set_feature_extractor(self, feature_extractor):\n",
        "    self.feature_extractor = feature_extractor\n",
        "    return self\n",
        "\n",
        "  def set_wav2vec2_model(self, wav2vec2_model):\n",
        "    self.wav2vec2_model = wav2vec2_model\n",
        "    return self\n",
        "\n",
        "  def set_emotion_classifier(self, emotion_classifier):\n",
        "    self.emotion_classifier = emotion_classifier\n",
        "    return self\n",
        "\n",
        "  def set_sampling_rate(self, sampling_rate):\n",
        "    self.sampling_rate = sampling_rate\n",
        "    return self\n",
        "\n",
        "  def set_device(self, device):\n",
        "    self.device = device\n",
        "    return self\n",
        "\n",
        "  def extract_features(self, wav_array, sampling_rate, device):\n",
        "    wavs_token = self.feature_extractor(raw_speech = [wav_array], sampling_rate = sampling_rate, do_normalize = True, return_tensors = 'pt').to(device)\n",
        "    outputs = self.wav2vec2_model(**wavs_token, output_hidden_states = True)\n",
        "    w2vlastfeat = outputs['last_hidden_state'].squeeze()\n",
        "    feature_array = torch.FloatTensor(w2vlastfeat)\n",
        "    feature_array = feature_array.to(device)\n",
        "    return feature_array\n",
        "\n",
        "  def forward(self, wav_array):\n",
        "    features = self.extract_features(wav_array, self.sampling_rate, self.device)\n",
        "    output = self.emotion_classifier(feature_array, feature_array.shape[0])\n",
        "    _, pred_label = torch.max(output.data, dim = 1)\n",
        "    return pred_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAglOA_wMI19"
      },
      "source": [
        "# Pre-processing stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNO5TPS0Absh"
      },
      "outputs": [],
      "source": [
        "##### NE KORISTI SE\n",
        "#\n",
        "##### THRESHOLDING PREDUGIH I PADDINGANJE PREKRATKIH ARRAYA FEATURA, KONVERTIRANJE ARRAYA U TENSOR\n",
        "#\n",
        "#def feature_array_padding(feature_array):\n",
        "#\n",
        "#  if np.shape(feature_array)[0] <= THRESHOLD:\n",
        "#    feature_array = np.pad(feature_array, ((0, THRESHOLD - np.shape(feature_array)[0]), (0, 0)), 'constant', constant_values=(0, 0))\n",
        "#  else:\n",
        "#    feature_array = feature_array[:THRESHOLD]\n",
        "#\n",
        "#  feature_array = torch.FloatTensor(feature_array)\n",
        "#  feature_array = torch.unsqueeze(feature_array, 0)\n",
        "#\n",
        "#  return feature_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q2TYDi-eyf7"
      },
      "outputs": [],
      "source": [
        "##### PAKIRANJE SAMPLE DATASET-A\n",
        "#\n",
        "#DIR = WAV2VEC2_LAST_TEST\n",
        "#files = os.listdir(DIR)\n",
        "#tar_SAMPLE = tarfile.open('/content/Wav2Vec2_SAMPLE.tar.gz', 'x:gz')\n",
        "#for file in tqdm(files):\n",
        "#   tar_SAMPLE.add(DIR + file, file)\n",
        "#tar_SAMPLE.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJupV-tjdh_"
      },
      "outputs": [],
      "source": [
        "#import shutil\n",
        "#\n",
        "#shutil.copy('/content/Wav2Vec2_SAMPLE.tar.gz', WAV2VEC2_SAM_PACKED)\n",
        "#shutil.copy('/content/sample_metadata.csv', SAMPLE_METADATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLUoHCobFgpU"
      },
      "outputs": [],
      "source": [
        "#import shutil\n",
        "#shutil.copy('/content/LSSED_Experiment/model_LSSED_Experiment_1.pt', DRIVE + 'model_LSSED_Experiment_1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OAAQeoaMixc"
      },
      "outputs": [],
      "source": [
        "##### NE POKRETATI - SPAJANJE DIJELOVA PREUZETOG DATASET-A NAZAD U CJELINU\n",
        "#\n",
        "# NIJE PYTHON NEGO CMD NAREDBA\n",
        "#\n",
        "#copy /B wav2vec2.tar.gz.0 + wav2vec2.tar.gz.1 + wav2vec2.tar.gz.2 D:\\wav2vec2.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLhyvUIWKpri"
      },
      "outputs": [],
      "source": [
        "##### NE POKRETATI - OTPAKIRAVANJE ORIGINALNOG DATASETA NA RAČUNALU\n",
        "#\n",
        "#file = tarfile.open(\"D:\\\\wav2vec2.tar.gz\")\n",
        "#members = file.getmembers()\n",
        "#progress = tqdm(members)\n",
        "#\n",
        "#for member in progress:\n",
        "#    file.extract(member, \"D:\\\\wav2vec2\")\n",
        "#    progress.set_description(f\"Extracting {member.name}\")\n",
        "#\n",
        "#file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqWbqzERJKOD"
      },
      "outputs": [],
      "source": [
        "##### NE POKRETATI - ČIŠĆENJE DATASETA OD EMOCIJA 'OTHER', 'FEAR', I 'SURPRISE'\n",
        "#\n",
        "#BASE_PATH = \"D:\\\\wav2vec2\\\\148Dataset\\\\data-fan.weiquan\\\\datasets\\\\1003\\\\feature\\\\wav2vec2\\\\\"\n",
        "#\n",
        "#df = pd.read_csv(\"D:\\\\wav2vec2_metadata.csv\")\n",
        "#removal = df[(df['Emotion'] == 'Other') | (df['Emotion'] == 'Fear') | (df['Emotion'] == 'Surprise')]['Vvid'].to_list()\n",
        "#print(removal.__len__())\n",
        "#\n",
        "#for item in tqdm(removal):\n",
        "#    os.remove(BASE_PATH+item)\n",
        "#    os.remove(BASE_PATH+item+\"_12\")\n",
        "#\n",
        "##### PROČIŠĆENO I OD 'unknown' DOBI - SADA POSTOJE SAMO 'Midlife', 'Younger', 'Old'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpkJ21stOqXS"
      },
      "outputs": [],
      "source": [
        "##### NE POKRETATI - NORMALIZACIJA DATASETA I PODJELA NA TRAIN, EVAL, TEST\n",
        "#\n",
        "#BASE_PATH = \"D:\\\\wav2vec2\\\\148Dataset\\\\data-fan.weiquan\\\\datasets\\\\1003\\\\feature\\\\wav2vec2\\\\\"\n",
        "#EMOTIONS = ['Angry', 'Bored', 'Disappointed', 'Disgusted', 'Excited', 'Happy', 'Neutral', 'Sad']\n",
        "#GENDERS = ['Female', 'Male']\n",
        "#AGES = ['Midlife', 'Old', 'Younger']\n",
        "#TRAIN_COUNTS = [1016, 344, 1015, 782, 176, 747, 420, 125, 555, 354, 67, 515, 1015, 542, 1015, 1015, 292, 1015, 508, 109, 735, 398, 75, 394, 782, 156, 1015, 561, 108, 767, 1015, 945, 1015, 1015, 634, 1015, 1015, 1015, 1015, 1015, 1015, 1015, 1015, 822, 1015, 1015, 369, 1015]\n",
        "#EVAL_COUNTS = [290, 98, 290, 223, 50, 213, 120, 35, 158, 101, 19, 147, 290, 155, 290, 290, 83, 290, 145, 31, 210, 113, 21, 112, 223, 44, 290, 160, 31, 219, 290, 270, 290, 290, 181, 290, 290, 290, 290, 290, 290, 290, 290, 235, 290, 290, 105, 290]\n",
        "#TEST_COUNTS = [145, 49, 145, 112, 25, 107, 60, 18, 79, 50, 9, 73, 145, 77, 145, 145, 42, 145, 72, 16, 105, 57, 11, 56, 112, 22, 145, 80, 15, 110, 145, 135, 145, 145, 90, 145, 145, 145, 145, 145, 145, 145, 145, 117, 145, 145, 53, 145]\n",
        "#\n",
        "#files = os.listdir(BASE_PATH)\n",
        "#df = pd.read_csv(\"D:\\\\wav2vec2_metadata.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
        "#\n",
        "#i = 1\n",
        "#training_vvids = np.array(sample(df[(df['Emotion'] == 'Angry') & (df['Gender'] == 'Female') & (df['Age'] == 'Midlife')]['Vvid'].to_list(), TRAIN_COUNTS[0]))\n",
        "#for emotion in EMOTIONS:\n",
        "#    for gender in GENDERS:\n",
        "#        for age in AGES:\n",
        "#            if not (emotion == 'Angry' and gender == 'Female' and age == 'Midlife'):\n",
        "#                training_vvids = np.append(training_vvids, np.array(sample(df[(df['Emotion'] == emotion) & (df['Gender'] == gender) & (df['Age'] == age)]['Vvid'].to_list(), TRAIN_COUNTS[i])))\n",
        "#                i += 1\n",
        "#training_df = df.loc[df['Vvid'].isin(training_vvids)].reset_index().drop(\"index\", axis = 1)\n",
        "#print(training_df)\n",
        "#print(training_df.describe())\n",
        "#training_df.to_csv(\"D:\\\\train_metadata.csv\")\n",
        "#\n",
        "#no_train_df = df.loc[df['Vvid'].isin([vvid for vvid in files if vvid not in training_vvids])]\n",
        "#eval_vvids = np.array(sample(no_train_df[(no_train_df['Emotion'] == 'Angry') & (no_train_df['Gender'] == 'Female') & (no_train_df['Age'] == 'Midlife')]['Vvid'].to_list(), EVAL_COUNTS[0]))\n",
        "#\n",
        "#i = 1\n",
        "#for emotion in EMOTIONS:\n",
        "#    for gender in GENDERS:\n",
        "#        for age in AGES:\n",
        "#            if not (emotion == 'Angry' and gender == 'Female' and age == 'Midlife'):\n",
        "#                eval_vvids = np.append(eval_vvids, np.array(sample(no_train_df[(no_train_df['Emotion'] == emotion) & (no_train_df['Gender'] == gender) & (no_train_df['Age'] == age)]['Vvid'].to_list(), EVAL_COUNTS[i])))\n",
        "#                i += 1\n",
        "#eval_df = no_train_df.loc[no_train_df['Vvid'].isin(eval_vvids)].reset_index().drop(\"index\", axis = 1)\n",
        "#print(eval_df)\n",
        "#print(eval_df.describe())\n",
        "#val_df.to_csv(\"D:\\\\eval_metadata.csv\")\n",
        "#\n",
        "#no_test_train_df = no_train_df.loc[no_train_df['Vvid'].isin([vvid for vvid in files if vvid not in eval_vvids])].reset_index()\n",
        "#test_vvids = np.array(sample(no_test_train_df[(no_test_train_df['Emotion'] == 'Angry') & (no_test_train_df['Gender'] == 'Female') & (no_test_train_df['Age'] == 'Midlife')]['Vvid'].to_list(), TEST_COUNTS[0]))\n",
        "#\n",
        "#i = 1\n",
        "#for emotion in EMOTIONS:\n",
        "#    for gender in GENDERS:\n",
        "#        for age in AGES:\n",
        "#            if not (emotion == 'Angry' and gender == 'Female' and age == 'Midlife'):\n",
        "#               test_vvids = np.append(test_vvids, np.array(sample(no_test_train_df[(no_test_train_df['Emotion'] == emotion) & (no_test_train_df['Gender'] == gender) & (no_test_train_df['Age'] == age)]['Vvid'].to_list(), TEST_COUNTS[i])))\n",
        "#               i += 1\n",
        "#test_df = no_test_train_df.loc[no_test_train_df['Vvid'].isin(test_vvids)].reset_index().drop(\"index\", axis = 1).drop(\"level_0\", axis = 1)\n",
        "#print(test_df)\n",
        "#print(test_df.describe())\n",
        "#test_df.to_csv(\"D:\\\\test_metadata.csv\")\n",
        "#\n",
        "#all_vvids = np.append(training_vvids, eval_vvids)\n",
        "#all_vvids = np.append(all_vvids, test_vvids)\n",
        "#print(np.shape(all_vvids))\n",
        "#\n",
        "#unused_vvids = df['Vvid'].to_numpy()\n",
        "#unused_vvids = np.setdiff1d(unused_vvids, all_vvids)\n",
        "#print(np.shape(unused_vvids))\n",
        "#\n",
        "#for item in tqdm(unused_vvids):\n",
        "#    os.remove(BASE_PATH+item)\n",
        "#    os.remove(BASE_PATH+item+\"_12\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac84TTrWeSwC"
      },
      "outputs": [],
      "source": [
        "##### NE POKRETATI - ANALIZIRANJE DATASETA\n",
        "#\n",
        "#BASE_PATH = \"D:\\\\wav2vec2\\\\148Dataset\\\\data-fan.weiquan\\\\datasets\\\\1003\\\\feature\\\\wav2vec2\\\\\"\n",
        "#\n",
        "#df = pd.concat([pd.read_csv(\"D:\\\\train_metadata.csv\"), pd.read_csv(\"D:\\\\eval_metadata.csv\"), pd.read_csv(\"D:\\\\test_metadata.csv\")]).reset_index()\n",
        "#print(df['Vvid'][0])\n",
        "#\n",
        "#new_el = loadmat(BASE_PATH + df['Vvid'][0])\n",
        "#new_el = [[element for element in upperElement] for upperElement in new_el['w2v2']]\n",
        "#lengths = np.array([new_el.__len__()])\n",
        "#\n",
        "#for i in tqdm(range(1, df.__len__())):\n",
        "#    new_el = loadmat(BASE_PATH + df['Vvid'][i])\n",
        "#    new_el = [[element for element in upperElement] for upperElement in new_el['w2v2']]\n",
        "#    lengths = np.append(lengths, new_el.__len__())\n",
        "#\n",
        "#np.save(\"D:\\\\filtered_dataset_feature_counts.npy\", lengths)\n",
        "#plt.hist(lengths, np.unique(lengths).__len__())\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2qNeEnNRpLm"
      },
      "outputs": [],
      "source": [
        "##### NE POKRETATI - PAKIRANJE OBRAĐENOG DATASETA ZA UPLOADANJE\n",
        "#\n",
        "#TRAIN = \"TRAIN\\\\\"\n",
        "#EVAL = \"EVAL\\\\\"\n",
        "#TEST = \"TEST\\\\\"\n",
        "#\n",
        "#WAV2VEC2_LAST = \"D:\\\\Wav2Vec2_LAST\\\\\"\n",
        "#W2V2_LAST_TRAIN = WAV2VEC2_LAST + TRAIN\n",
        "#W2V2_LAST_EVAL = WAV2VEC2_LAST + EVAL\n",
        "#W2V2_LAST_TEST = WAV2VEC2_LAST + TEST\n",
        "#W2V2_LAST_DIRECTORIES = [W2V2_LAST_TRAIN, W2V2_LAST_EVAL, W2V2_LAST_TEST]\n",
        "#\n",
        "#WAV2VEC2_12 = \"D:\\\\Wav2Vec2_12\\\\\"\n",
        "#W2V2_12_TRAIN = WAV2VEC2_12 + TRAIN\n",
        "#W2V2_12_EVAL = WAV2VEC2_12 + EVAL\n",
        "#W2V2_12_TEST = WAV2VEC2_12 + TEST\n",
        "#W2V2_12_DIRECTORIES = [W2V2_12_TRAIN, W2V2_12_EVAL, W2V2_12_TEST]\n",
        "#\n",
        "#tar_12 = tar.open(\"D:\\\\Wav2Vec2_12.tar.gz\", 'x:gz')\n",
        "#for dir in W2V2_12_DIRECTORIES:\n",
        "#    files = os.listdir(dir)\n",
        "#    print(\"Processing \" + dir + \" ...\")\n",
        "#    for file in tqdm(files):\n",
        "#        tar_12.add(dir + file)\n",
        "#tar_12.close()\n",
        "#\n",
        "#tar_LAST = tar.open(\"D:\\\\Wav2Vec2_LAST.tar.gz\", 'x:gz')\n",
        "#for dir in W2V2_LAST_DIRECTORIES:\n",
        "#    files = os.listdir(dir)\n",
        "#    print(\"Processing \" + dir + \" ...\")\n",
        "#    for file in tqdm(files):\n",
        "#        tar_LAST.add(dir + file)\n",
        "#tar_LAST.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}