{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOWgqu8COgNGCTJltIAlEow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-antimonious/GRU-Emotion-Classification/blob/main/RUSU_testiranje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVfuPQ9q8l14"
      },
      "outputs": [],
      "source": [
        "##### IMPORTI, KONSTANTE, UTILITIES\n",
        "\n",
        "%%capture\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import shutil\n",
        "import math\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import librosa\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "import seaborn\n",
        "\n",
        "##### PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "\n",
        "DRIVE =         '/content/drive/MyDrive/LSSED/'   # KONSTANTA ZA DRIVE FOLDER GDJE JE POHRANJENO SVE ZA PROJEKT\n",
        "TEST_METADATA = DRIVE + 'test_metadata.csv'       # KONSTANTA ZA TESTING DATA FILE\n",
        "SAMPLING_RATE = 16000                             # KONSTANTA ZA SAMPLING RATE SNIMAKA\n",
        "BATCH_SIZE =    1                                 # KONSTANTA ZA VELIÄŒINU BATCHA\n",
        "DEVICE =        torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "WAV2VEC2_NAME =     'facebook/wav2vec2-large-xlsr-53'\n",
        "FEATURE_EXTRACTOR = transformers.Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_NAME)\n",
        "WAV2VEC2_MODEL =    transformers.Wav2Vec2Model.from_pretrained(WAV2VEC2_NAME).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "shutil.copy('/content/drive/MyDrive/kaggle.json', '/content/kaggle.json')\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dmitrybabko/speech-emotion-recognition-en\n",
        "!unzip speech-emotion-recognition-en.zip -d speech-emotion-recognition-en"
      ],
      "metadata": {
        "id": "WhZFxGE48qUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CREMA_DIR = '/content/speech-emotion-recognition-en/Crema/'\n",
        "RAVDESS_DIR = '/content/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24/'\n",
        "SAVEE_DIR = '/content/speech-emotion-recognition-en/Savee/'\n",
        "TESS_DIR = '/content/speech-emotion-recognition-en/Tess/'\n",
        "\n",
        "RAVDESS_ACT_DIRS = os.listdir(RAVDESS_DIR)\n",
        "TESS_ACT_DIRS = os.listdir(TESS_DIR)\n",
        "\n",
        "NEW_DIR = '/content/speech-emotion-recognition-en/'\n",
        "\n",
        "file_index = 0\n",
        "\n",
        "for f in tqdm(os.listdir(CREMA_DIR)):\n",
        "  emotion = f.split('_')[2]\n",
        "\n",
        "  match emotion:\n",
        "    case 'SAD':\n",
        "      new_emotion = 'sadness'\n",
        "    case 'ANG':\n",
        "      new_emotion = 'anger'\n",
        "    case 'HAP':\n",
        "      new_emotion = 'happiness'\n",
        "    case 'NEU':\n",
        "      new_emotion = 'neutral'\n",
        "    case 'DIS':\n",
        "      new_emotion = 'disgust'\n",
        "    case _ :\n",
        "      continue\n",
        "\n",
        "  new_name = new_emotion + '_' + str(file_index) + '.wav'\n",
        "  os.rename(CREMA_DIR + f, NEW_DIR + new_name)\n",
        "  file_index = file_index + 1\n",
        "\n",
        "for d in tqdm(RAVDESS_ACT_DIRS):\n",
        "  for f in os.listdir(RAVDESS_DIR + d):\n",
        "    emotion = f.split('-')[2]\n",
        "\n",
        "    match emotion:\n",
        "      case '01':\n",
        "        new_emotion = 'neutral'\n",
        "      case '03':\n",
        "        new_emotion = 'happiness'\n",
        "      case '04':\n",
        "        new_emotion = 'sadness'\n",
        "      case '05':\n",
        "        new_emotion = 'anger'\n",
        "      case '07':\n",
        "        new_emotion = 'disgust'\n",
        "      case _ :\n",
        "        continue\n",
        "\n",
        "    new_name = new_emotion + '_' + str(file_index) + '.wav'\n",
        "    os.rename(RAVDESS_DIR + d + '/' + f, NEW_DIR + new_name)\n",
        "    file_index = file_index + 1\n",
        "\n",
        "for d in tqdm(TESS_ACT_DIRS):\n",
        "  for f in os.listdir(TESS_DIR + d):\n",
        "    emotion = f.split('_')[2].split('.')[0]\n",
        "\n",
        "    match emotion:\n",
        "      case 'sad':\n",
        "        new_emotion = 'sadness'\n",
        "      case 'angry':\n",
        "        new_emotion = 'anger'\n",
        "      case 'disgust':\n",
        "        new_emotion = 'disgust'\n",
        "      case 'happy':\n",
        "        new_emotion = 'happiness'\n",
        "      case 'neutral':\n",
        "        new_emotion = 'neutral'\n",
        "      case _ :\n",
        "        continue\n",
        "\n",
        "    new_name = new_emotion + '_' + str(file_index) + '.wav'\n",
        "    os.rename(TESS_DIR + d + '/' + f, NEW_DIR + new_name)\n",
        "    file_index = file_index + 1\n",
        "\n",
        "for f in tqdm(os.listdir(SAVEE_DIR)):\n",
        "  emotion = f.split('_')[1].split('.')[0]\n",
        "\n",
        "  match emotion[0]:\n",
        "    case 'a':\n",
        "      new_emotion = 'anger'\n",
        "    case 'd':\n",
        "      new_emotion = 'disgust'\n",
        "    case 'h':\n",
        "      new_emotion = 'happiness'\n",
        "    case 'n':\n",
        "      new_emotion = 'neutral'\n",
        "    case 's':\n",
        "      match emotion[1]:\n",
        "        case 'a':\n",
        "          new_emotion = 'sadness'\n",
        "        case _ :\n",
        "          continue\n",
        "    case _ :\n",
        "      continue\n",
        "\n",
        "  new_name = new_emotion + '_' + str(file_index) + '.wav'\n",
        "  os.rename(SAVEE_DIR + f, NEW_DIR + new_name)\n",
        "  file_index = file_index + 1"
      ],
      "metadata": {
        "id": "i5F9QX1Z8sc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in tqdm(os.listdir(CREMA_DIR)):\n",
        "  os.remove(CREMA_DIR + f)\n",
        "os.rmdir(CREMA_DIR)\n",
        "\n",
        "for d in tqdm(RAVDESS_ACT_DIRS):\n",
        "  for f in os.listdir(RAVDESS_DIR + d):\n",
        "    os.remove(RAVDESS_DIR + d + '/' + f)\n",
        "  os.rmdir(RAVDESS_DIR + d)\n",
        "os.rmdir(RAVDESS_DIR)\n",
        "os.rmdir('/content/speech-emotion-recognition-en/Ravdess')\n",
        "\n",
        "for d in tqdm(TESS_ACT_DIRS):\n",
        "  for f in os.listdir(TESS_DIR + d):\n",
        "    os.remove(TESS_DIR + d + '/' + f)\n",
        "  os.rmdir(TESS_DIR + d)\n",
        "os.rmdir(TESS_DIR)\n",
        "\n",
        "for f in tqdm(os.listdir(SAVEE_DIR)):\n",
        "  os.remove(SAVEE_DIR + f)\n",
        "os.rmdir(SAVEE_DIR)"
      ],
      "metadata": {
        "id": "nYD5a9pm8uhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = os.listdir(NEW_DIR)\n",
        "\n",
        "anger = [fi for fi in f if fi.split('_')[0] == 'anger']\n",
        "sadness = [fi for fi in f if fi.split('_')[0] == 'sadness']\n",
        "disgust = [fi for fi in f if fi.split('_')[0] == 'disgust']\n",
        "happiness = [fi for fi in f if fi.split('_')[0] == 'happiness']\n",
        "neutral = [fi for fi in f if fi.split('_')[0] == 'neutral']\n",
        "\n",
        "emotions = [anger, sadness, disgust, happiness, neutral]\n",
        "emotion_names = ['anger', 'sadness', 'disgust', 'happiness', 'neutral']\n",
        "\n",
        "try:\n",
        "  os.mkdir(NEW_DIR + 'test/')\n",
        "except Exception as ex:\n",
        "  print(ex)\n",
        "\n",
        "test = pd.DataFrame(columns = ['File', 'Emotion'])\n",
        "\n",
        "for i in range(emotions.__len__()):\n",
        "  test_temp = pd.DataFrame({'File': emotions[i], 'Emotion': np.repeat(emotion_names[i], emotions[i].__len__())})\n",
        "  test = pd.concat([test, test_temp]).reset_index().drop('index', axis = 1)\n",
        "\n",
        "print(test)"
      ],
      "metadata": {
        "id": "NX1EMZfj8wf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for te in tqdm(test['File'].to_list()):\n",
        "  os.rename(NEW_DIR + te, NEW_DIR + 'test/' + te)"
      ],
      "metadata": {
        "id": "I6GQJwSy8zBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['Emotion'] = test['Emotion'].astype('category')\n",
        "\n",
        "encoded_test =  pd.get_dummies(test['Emotion'])\n",
        "\n",
        "test = pd.concat([test, encoded_test], axis = 1)"
      ],
      "metadata": {
        "id": "ZCSCa8aA81XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "metadata": {
        "id": "OooIXPIl9PDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Speech_Dataset(Dataset):\n",
        "  \"\"\"Speech recordings dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, metadata, directory, transform = None):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        metadata (DataFrame):             Pandas DataFrame containing dataset information.\n",
        "        directory (string):               Path to the directory with the feature array files.\n",
        "        transform (class | list | None):  Data transformation options.\n",
        "    \"\"\"\n",
        "    self.metadata = metadata\n",
        "    self.directory = directory\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.metadata)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    path = self.directory + self.metadata['File'][idx]\n",
        "    emotion = [self.metadata['anger'][idx], self.metadata['happiness'][idx], self.metadata['neutral'][idx], self.metadata['sadness'][idx], self.metadata['disgust'][idx]]\n",
        "\n",
        "    if self.transform:\n",
        "      recording = self.transform(recording)\n",
        "\n",
        "    sample = {'path': path, 'emotion': emotion}\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "qCjnW-JC9Dhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Speech_Dataset(test, NEW_DIR + 'test/')"
      ],
      "metadata": {
        "id": "_UibDOz99Rw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "  def load_recording(path):\n",
        "    recording = librosa.load(path, sr = SAMPLING_RATE)[0]\n",
        "    return recording\n",
        "\n",
        "  data = load_recording(batch[0]['path'])\n",
        "  target = batch[0]['emotion']\n",
        "\n",
        "  target = torch.as_tensor(target)\n",
        "\n",
        "  return (data, target)"
      ],
      "metadata": {
        "id": "gmb0wZ9Q9SFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, False, num_workers = 12, collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "yAfotA1Z-R9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### DEFINIRANJE KLASIFIKATORA\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm = nn.LayerNorm(normalized_shape = 1024)\n",
        "    self.rnn1 = nn.LSTM(input_size = 1024, hidden_size = 1024, num_layers = 3, batch_first = True, bidirectional = False)\n",
        "    self.linear1 = nn.Linear(1024, 5)\n",
        "\n",
        "  def forward(self, x, length):\n",
        "    out = self.norm(x)\n",
        "    out, _ = self.rnn1(out)\n",
        "\n",
        "    # Many-to-one RNN mod\n",
        "    try:\n",
        "      _ = out.shape[2]\n",
        "      indices = [i for i in range(out.shape[0])]\n",
        "      out = out[indices, np.subtract(length, 1), :]\n",
        "    except:\n",
        "      out = out[np.subtract(length, 1), :]\n",
        "\n",
        "    out = self.linear1(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DYes3vd9-Tez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### DEFINIRANJE CIJELOG MODELA\n",
        "\n",
        "class Emotioner(nn.Module):\n",
        "  def __init__(self, feature_extractor, wav2vec2_model, emotion_classifier, sampling_rate = 16000):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.wav2vec2_model = wav2vec2_model\n",
        "    self.emotion_classifier = emotion_classifier\n",
        "    self.sampling_rate = sampling_rate\n",
        "\n",
        "  def set_feature_extractor(self, feature_extractor):\n",
        "    self.feature_extractor = feature_extractor\n",
        "    return self\n",
        "\n",
        "  def set_wav2vec2_model(self, wav2vec2_model):\n",
        "    self.wav2vec2_model = wav2vec2_model\n",
        "    return self\n",
        "\n",
        "  def set_emotion_classifier(self, emotion_classifier):\n",
        "    self.emotion_classifier = emotion_classifier\n",
        "    return self\n",
        "\n",
        "  def set_sampling_rate(self, sampling_rate):\n",
        "    self.sampling_rate = sampling_rate\n",
        "    return self\n",
        "\n",
        "  def extract_features(self, wav_array, sampling_rate):\n",
        "    wavs_token = self.feature_extractor([wav_array], sampling_rate = sampling_rate, padding = True, do_normalize = True, return_tensors = 'pt').to(DEVICE)\n",
        "    outputs = self.wav2vec2_model(**wavs_token, output_hidden_states = True)\n",
        "    w2vlastfeat = outputs['last_hidden_state'].squeeze().detach().cpu().numpy()\n",
        "    feature_array = torch.FloatTensor(w2vlastfeat).to(DEVICE)\n",
        "    return feature_array\n",
        "\n",
        "  def forward(self, wav_array):\n",
        "    features = self.extract_features(wav_array, self.sampling_rate)\n",
        "    output = self.emotion_classifier(features, features.shape[0])\n",
        "    _, pred_label = torch.max(output.data, dim = 0)\n",
        "    return (output, pred_label)"
      ],
      "metadata": {
        "id": "Pm4d23rl-ZUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### RNN TESTING\n",
        "\n",
        "y_pred = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
        "y_true = []\n",
        "\n",
        "true_preds, num_preds, epoch_acc = np.zeros(12), np.zeros(12), np.zeros(12)\n",
        "true_zeros, true_ones, true_twos, true_threes, true_fours = np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12) # TP\n",
        "false_zeros, false_ones, false_twos, false_threes, false_fours = np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12) # FP\n",
        "missed_zeros, missed_ones, missed_twos, missed_threes, missed_fours = np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12) # FN\n",
        "\n",
        "prec_zeros, prec_ones, prec_twos, prec_threes, prec_fours = np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12)\n",
        "rec_zeros, rec_ones, rec_twos, rec_threes, rec_fours = np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12)\n",
        "f1_zeros, f1_ones, f1_twos, f1_threes, f1_fours = np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12), np.zeros(12)\n",
        "\n",
        "epoch_rec, epoch_prec, epoch_f1 = np.zeros(12), np.zeros(12), np.zeros(12)"
      ],
      "metadata": {
        "id": "yVH0yzArOUCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mod(models, test_data_loader):\n",
        "  global y_pred, y_true, true_preds, num_preds, epoch_acc, true_zeros, true_ones, true_twos, true_threes, true_fours, false_zeros, false_ones, false_twos, false_threes, false_fours, \\\n",
        "  missed_zeros, missed_ones, missed_twos, missed_threes, missed_fours, prec_zeros, prec_ones, prec_twos, prec_threes, prec_fours, rec_zeros, rec_ones, rec_twos, rec_threes, rec_fours, \\\n",
        "  f1_zeros, f1_ones, f1_twos, f1_threes, f1_fours, epoch_rec, epoch_prec, epoch_f1\n",
        "\n",
        "  for data_inputs, labels in tqdm(test_data_loader):\n",
        "\n",
        "    ##### Moving data to device\n",
        "    _, labels = torch.max(labels.data, dim = 0)\n",
        "    y_true = np.append(y_true, labels)\n",
        "\n",
        "    for i in range(models.__len__()):\n",
        "      output, pred_labels = models[i](data_inputs)\n",
        "\n",
        "      true_preds[i] += (pred_labels == labels).sum().item()\n",
        "\n",
        "      true_zeros[i] +=   torch.sum((pred_labels == 0) & (labels == 0)).item()\n",
        "      true_ones[i] +=    torch.sum((pred_labels == 1) & (labels == 1)).item()\n",
        "      true_twos[i] +=    torch.sum((pred_labels == 2) & (labels == 2)).item()\n",
        "      true_threes[i] +=  torch.sum((pred_labels == 3) & (labels == 3)).item()\n",
        "      true_fours[i] +=   torch.sum((pred_labels == 4) & (labels == 4)).item()\n",
        "\n",
        "      false_zeros[i] +=  torch.sum((pred_labels == 0) & (labels != 0)).item()\n",
        "      false_ones[i] +=   torch.sum((pred_labels == 1) & (labels != 1)).item()\n",
        "      false_twos[i] +=   torch.sum((pred_labels == 2) & (labels != 2)).item()\n",
        "      false_threes[i] += torch.sum((pred_labels == 3) & (labels != 3)).item()\n",
        "      false_fours[i] +=  torch.sum((pred_labels == 4) & (labels != 4)).item()\n",
        "\n",
        "      missed_zeros[i] +=   torch.sum((pred_labels != 0) & (labels == 0)).item()\n",
        "      missed_ones[i] +=    torch.sum((pred_labels != 1) & (labels == 1)).item()\n",
        "      missed_twos[i] +=    torch.sum((pred_labels != 2) & (labels == 2)).item()\n",
        "      missed_threes[i] +=  torch.sum((pred_labels != 3) & (labels == 3)).item()\n",
        "      missed_fours[i] +=   torch.sum((pred_labels != 4) & (labels == 4)).item()\n",
        "\n",
        "      num_preds[i] += 1\n",
        "      y_pred[i] = np.append(y_pred[i], pred_labels.cpu().numpy())\n",
        "\n",
        "      del output, pred_labels\n",
        "\n",
        "    ##### Cleaning up\n",
        "    del data_inputs, labels\n",
        "\n",
        "  ##### Metrics\n",
        "  for i in range(models.__len__()):\n",
        "    epoch_acc[i] =   true_preds[i] / num_preds[i]\n",
        "\n",
        "    zeros_weight =  1923. / len(test_data_loader.dataset)\n",
        "    ones_weight =   1923. / len(test_data_loader.dataset)\n",
        "    twos_weight =   1703. / len(test_data_loader.dataset)\n",
        "    threes_weight = 1923. / len(test_data_loader.dataset)\n",
        "    fours_weight =  1923. / len(test_data_loader.dataset)\n",
        "\n",
        "    prec_zeros[i] =  true_zeros[i] / (true_zeros[i] + false_zeros[i] + 1e-10)\n",
        "    prec_ones[i] =   true_ones[i] / (true_ones[i] + false_ones[i] + 1e-10)\n",
        "    prec_twos[i] =   true_twos[i] / (true_twos[i] + false_twos[i] + 1e-10)\n",
        "    prec_threes[i] = true_threes[i] / (true_threes[i] + false_threes[i] + 1e-10)\n",
        "    prec_fours[i] =  true_fours[i] / (true_fours[i] + false_fours[i] + 1e-10)\n",
        "\n",
        "    rec_zeros[i] =   true_zeros[i] / (true_zeros[i] + missed_zeros[i] + 1e-10)\n",
        "    rec_ones[i] =    true_ones[i] / (true_ones[i] + missed_ones[i] + 1e-10)\n",
        "    rec_twos[i] =    true_twos[i] / (true_twos[i] + missed_twos[i] + 1e-10)\n",
        "    rec_threes[i] =  true_threes[i] / (true_threes[i] + missed_threes[i] + 1e-10)\n",
        "    rec_fours[i] =   true_fours[i] / (true_fours[i] + missed_fours[i] + 1e-10)\n",
        "\n",
        "    f1_zeros[i] =  (2 * prec_zeros[i] * rec_zeros[i]) / (prec_zeros[i] + rec_zeros[i] + 1e-10)\n",
        "    f1_ones[i] =   (2 * prec_ones[i] * rec_ones[i]) / (prec_ones[i] + rec_ones[i] + 1e-10)\n",
        "    f1_twos[i] =   (2 * prec_twos[i] * rec_twos[i]) / (prec_twos[i] + rec_twos[i] + 1e-10)\n",
        "    f1_threes[i] = (2 * prec_threes[i] * rec_threes[i]) / (prec_threes[i] + rec_threes[i] + 1e-10)\n",
        "    f1_fours[i] =  (2 * prec_fours[i] * rec_fours[i]) / (prec_fours[i] + rec_fours[i] + 1e-10)\n",
        "\n",
        "    epoch_prec[i] =  zeros_weight * prec_zeros[i] + ones_weight * prec_ones[i] + twos_weight * prec_twos[i] + threes_weight * prec_threes[i] + fours_weight * prec_fours[i]\n",
        "    epoch_rec[i] =   zeros_weight * rec_zeros[i] + ones_weight * rec_ones[i] + twos_weight * rec_twos[i] + threes_weight * rec_threes[i] + fours_weight * rec_fours[i]\n",
        "    epoch_f1[i] =    zeros_weight * f1_zeros[i] + ones_weight * f1_ones[i] + twos_weight * f1_twos[i] + threes_weight * f1_threes[i] + fours_weight * f1_fours[i]\n",
        "\n",
        "    print(\"Model\", str(i), \":\")\n",
        "    print(f\"Testing accuracy: {100.0*epoch_acc[i]:4.5f}%\")\n",
        "    print(f\"Testing precision: {100.0*epoch_prec[i]:4.5f}%\")\n",
        "    print(f\"Testing recall: {100.0*epoch_rec[i]:4.5f}%\")\n",
        "    print(f\"Testing F1-score: {100.0*epoch_f1[i]:4.5f}%\")\n",
        "\n",
        "    print(\"Testing zeros:\", str(true_zeros[i] + false_zeros[i]))\n",
        "    print(\"Testing ones:\", str(true_ones[i] + false_ones[i]))\n",
        "    print(\"Testing twos:\", str(true_twos[i] + false_twos[i]))\n",
        "    print(\"Testing threes:\", str(true_threes[i] + false_threes[i]))\n",
        "    print(\"Testing fours:\", str(true_fours[i] + false_fours[i]))"
      ],
      "metadata": {
        "id": "bn5bQ9X2-zYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = os.listdir(DRIVE + 'LSTM_MODELS')\n",
        "model_names.sort()\n",
        "\n",
        "models = []\n",
        "for name in tqdm(model_names):\n",
        "  temp_mod = EmotionClassifier().to(DEVICE)\n",
        "  temp_mod.load_state_dict(torch.load(DRIVE + 'LSTM_MODELS/' + name))\n",
        "  temp_mod.eval()\n",
        "  mod = Emotioner(FEATURE_EXTRACTOR, WAV2VEC2_MODEL, temp_mod, SAMPLING_RATE).to(DEVICE)\n",
        "  mod.eval()\n",
        "  models = np.append(models, mod)"
      ],
      "metadata": {
        "id": "cM_z8tl5JGzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mod(models, test_dataloader)"
      ],
      "metadata": {
        "id": "e3eTOHGOMlo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(17, 29)\n",
        "\n",
        "for i in range(models.__len__()):\n",
        "  plt.figure()\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_true, y_pred[i]), display_labels = ['anger', 'happiness', 'neutral', 'sadness', 'disgust'])\n",
        "  disp.plot()\n",
        "  plt.savefig('confusion_matrix_rnn_' + str(epochs[i]) + '_.png')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xsTBWxAXxt8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in epochs:\n",
        "  shutil.copy('/content/confusion_matrix_rnn_' + str(e) +'_.png', DRIVE + 'LSTM_CONFUSION/confusion_matrix_lstm_' + str(e) + '.png')"
      ],
      "metadata": {
        "id": "eiVSlmwj0Gr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}